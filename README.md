Semantic wireless communication plays a pivotal role in next-generation mobile network applications that emphasize multiple performance metrics such as latency and energy consumption. However, existing semantic-aware source coding schemes face challenges in meeting the requirements of high-quality communication, manifested in two critical aspects: the inefficient fusion of heterogeneous semantic features across multimodal data (e.g., text, speech, images, and videos) leading to compromised cross-modal semantic consistency, and the noise-sensitive nature of semantic codecs that potentially induces semantic ambiguity propagation. This study addresses these challenges by developing a multimodal semantic codec framework, which employs lightweight artificial intelligence models tailored for multimodal data processing. The proposed approach aims to optimize the trade-offs among compression efficiency, power consumption, and transmission latency through systematic architectural innovations.
